{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "\n",
    "In this notebook, we will perform an EDA (Exploratory Data Analysis) on the processed Waymo dataset (data in the `processed` folder). In the first part, you will create a function to display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "HWC = namedtuple(\"HWC\", [\"h\", \"w\", \"c\"])\n",
    "HWC.__new__.__defaults__ = (0, 0, 0)\n",
    "HWC.__str__ = lambda d: f\"HWC({d.h},{d.w},{d.c})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "COLORMAP = {1: \"red\", 2: \"blue\", 4: \"green\"}\n",
    "MAPPING_CLASS = {1: \"vehicles\", 2: \"pedestrian\", 4: \"cyclist\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"data/waymo/training_and_validation/*.tfrecord\")\n",
    "#dataset = get_dataset(\"data/waymo/train/*.tfrecord\")\n",
    "#dataset = get_dataset(\"data/waymo/validation/*.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display an image and the bounding boxes\n",
    "\n",
    "Implement the `display_instances` function below. This function takes a batch as an input and display an image with its corresponding bounding boxes. The only requirement is that the classes should be color coded (eg, vehicles in red, pedestrians in blue, cyclist in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_instances(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch from the dataset and display the image with \n",
    "    the associated bounding boxes.\n",
    "    \"\"\"\n",
    "    image = batch[\"image\"].numpy()\n",
    "    image_size = HWC(*image.shape)\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    bboxes = batch[\"groundtruth_boxes\"].numpy()\n",
    "    classes = batch[\"groundtruth_classes\"].numpy()\n",
    "    for cl, bb in zip(classes, bboxes):\n",
    "        y1, x1, y2, x2 = bb\n",
    "        y1, y2 = int(y1*image_size.h), int(y2*image_size.h)\n",
    "        x1, x2 = int(x1*image_size.w), int(x2*image_size.w)\n",
    "        rec = Rectangle((x1, y1), x2- x1, y2-y1, facecolor='none', edgecolor=COLORMAP[cl])\n",
    "        ax.add_patch(rec)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(batch[\"filename\"].numpy().decode(), color = \"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 10 images \n",
    "\n",
    "Using the dataset created in the second cell and the function you just coded, display 10 random images with the associated bounding boxes. You can use the methods `take` and `shuffle` on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "BATCH_SIZE_DISPLAY = 10\n",
    "batch = dataset.shuffle(BATCH_SIZE_DISPLAY, reshuffle_each_iteration=True)\n",
    "plt.figure()\n",
    "for idx in np.arange(BATCH_SIZE_DISPLAY):\n",
    "    for t in batch.take(1):\n",
    "        display_instances(t)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA\n",
    "\n",
    "In this last part, you are free to perform any additional analysis of the dataset. What else would like to know about the data?\n",
    "For example, think about data distribution. So far, you have only looked at a single file...\n",
    "\n",
    "Due to the size of the dataset, we will focus an a restricted batch of images(defined by the user) to perform data analysys such as:\n",
    "\n",
    ">- Class Distribution: used to identify if a class is more represented in the dataset. If yes, may or may not require a class compensation through Data Augmentation or Data Acquisition, depending of the use case.\n",
    ">- Number of instances/object per image: Allow to know if the dataset is based on scenes with high density objects and further to have better train/validation split, data augmentation, etc.\n",
    ">- Number of each class per image: can be usefull for data augmentation\n",
    ">- Image Brightness: can be usefull for data augmentation, weather robustness, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_ANALYSIS = 20000\n",
    "\n",
    "distri = {1: [], 2: [], 4: []}\n",
    "nb_obj_per_img = {\"max\" : 0, \"mean\" : 0, \"min\": 0}\n",
    "brightness = {\"max\" : [], \"mean\" : [], \"min\": []}\n",
    "from PIL import Image\n",
    "id = 0\n",
    "for idx,batch in enumerate(dataset.take(BATCH_SIZE_ANALYSIS)):\n",
    "    img = batch[\"image\"].numpy()\n",
    "    PIL_image = np.asarray(Image.fromarray(np.uint8(img)).convert('L'))\n",
    "    brightness[\"max\"].append(np.max(PIL_image))\n",
    "    brightness[\"min\"].append(np.min(PIL_image))\n",
    "    brightness[\"mean\"].append(np.mean(PIL_image))\n",
    "\n",
    "    classes = batch[\"groundtruth_classes\"].numpy().tolist()\n",
    "\n",
    "    for idx_distri in distri.keys():\n",
    "        distri[idx_distri].append(classes.count(idx_distri))\n",
    "\n",
    "    nb_obj_per_img[\"max\"] = max(nb_obj_per_img[\"max\"], len(classes))\n",
    "    nb_obj_per_img[\"min\"] = min(nb_obj_per_img[\"min\"], len(classes))\n",
    "    nb_obj_per_img[\"mean\"] = (((nb_obj_per_img[\"mean\"] + len(classes)) / 2) if idx!=0 else len(classes))\n",
    "    id += 1\n",
    "\n",
    "print(\"How many objects/instances per Image:\")\n",
    "print(f\"         Maximum Nb of Instances: {nb_obj_per_img['max']}\")\n",
    "print(f\"         Average Nb of Instances: {nb_obj_per_img['mean']}\")\n",
    "print(f\"         Minimum Nb of Instances: {nb_obj_per_img['min']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "print(f\"Mean Brightness : {np.mean(brightness['mean'])}\")\n",
    "fig.patch.set_facecolor('white')\n",
    "x = np.arange(len(brightness[\"mean\"]))\n",
    "plt.hist(brightness[\"mean\"],max(brightness[\"mean\"]).astype(np.int32))\n",
    "\n",
    "plt.ylabel('Nb images', fontsize=18)\n",
    "plt.title(f'Luminance Distribution over {BATCH_SIZE_ANALYSIS}', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.patch.set_facecolor('white')\n",
    "for idx, name in MAPPING_CLASS.items():\n",
    "    plt.bar(name, sum(distri[idx]), align='center', alpha=0.5, color=COLORMAP[idx])\n",
    "    print(f\"{name} represents {sum(distri[idx])}/{BATCH_SIZE_ANALYSIS} = {sum(distri[idx])*100/BATCH_SIZE_ANALYSIS}%\")\n",
    "\n",
    "plt.xticks(np.arange(len(MAPPING_CLASS.values())), MAPPING_CLASS.values(), fontsize=18)\n",
    "plt.ylabel('Instances', fontsize=18)\n",
    "plt.title(f'Classes Distribution over {BATCH_SIZE_ANALYSIS}', fontsize=18)\n",
    "tmp=[sum(distri[idx]) for idx in MAPPING_CLASS.keys()]\n",
    "for i, v in enumerate(tmp):\n",
    "    plt.text(i-.15, v/tmp[i]+100, f\"{tmp[i]} - {sum(distri[idx])*100/BATCH_SIZE_ANALYSIS}%\", fontsize=18, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,len(distri.keys()), figsize=(16,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "for idx in np.arange(len(distri.keys())):\n",
    "    key = list(distri.keys())[idx]\n",
    "    ax[idx].hist(distri[key], max(distri[key]))\n",
    "    ax[idx].set_title(MAPPING_CLASS[key])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
