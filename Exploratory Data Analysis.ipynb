{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "\n",
    "In this notebook, we will perform an EDA (Exploratory Data Analysis) on the processed Waymo dataset (data in the `processed` folder). In the first part, you will create a function to display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "HWC = namedtuple(\"HWC\", [\"h\", \"w\", \"c\"])\n",
    "HWC.__new__.__defaults__ = (0, 0, 0)\n",
    "HWC.__str__ = lambda d: f\"HWC({d.h},{d.w},{d.c})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "COLORMAP = {1: \"red\", 2: \"blue\", 4: \"green\"}\n",
    "MAPPING_CLASS = {1: \"vehicles\", 2: \"pedestrian\", 4: \"cyclist\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading unweighted datasets: ['data/waymo/training_and_validation/*.tfrecord']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:29:00.358752: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading record datasets for input file: ['data/waymo/training_and_validation/*.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 97\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(\"data/waymo/training_and_validation/*.tfrecord\")\n",
    "#dataset = get_dataset(\"data/waymo/train/*.tfrecord\")\n",
    "#dataset = get_dataset(\"data/waymo/validation/*.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display an image and the bounding boxes\n",
    "\n",
    "Implement the `display_instances` function below. This function takes a batch as an input and display an image with its corresponding bounding boxes. The only requirement is that the classes should be color coded (eg, vehicles in red, pedestrians in blue, cyclist in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_instances(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch from the dataset and display the image with \n",
    "    the associated bounding boxes.\n",
    "    \"\"\"\n",
    "    image = batch[\"image\"].numpy()\n",
    "    image_size = HWC(*image.shape)\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    bboxes = batch[\"groundtruth_boxes\"].numpy()\n",
    "    classes = batch[\"groundtruth_classes\"].numpy()\n",
    "    for cl, bb in zip(classes, bboxes):\n",
    "        y1, x1, y2, x2 = bb\n",
    "        y1, y2 = int(y1*image_size.h), int(y2*image_size.h)\n",
    "        x1, x2 = int(x1*image_size.w), int(x2*image_size.w)\n",
    "        rec = Rectangle((x1, y1), x2- x1, y2-y1, facecolor='none', edgecolor=COLORMAP[cl])\n",
    "        ax.add_patch(rec)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(batch[\"filename\"].numpy().decode(), color = \"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 10 images \n",
    "\n",
    "Using the dataset created in the second cell and the function you just coded, display 10 random images with the associated bounding boxes. You can use the methods `take` and `shuffle` on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "data/waymo/training_and_validation/segment-11967272535264406807_580_000_600_000_with_camera_labels.tfrecord; Input/output error [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/app/project/Exploratory Data Analysis.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f656c6567616e745f6d617473756d6f746f227d/app/project/Exploratory%20Data%20Analysis.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f656c6567616e745f6d617473756d6f746f227d/app/project/Exploratory%20Data%20Analysis.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(BATCH_SIZE_DISPLAY):\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f656c6567616e745f6d617473756d6f746f227d/app/project/Exploratory%20Data%20Analysis.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f656c6567616e745f6d617473756d6f746f227d/app/project/Exploratory%20Data%20Analysis.ipynb#ch0000008vscode-remote?line=6'>7</a>\u001b[0m         display_instances(t)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f656c6567616e745f6d617473756d6f746f227d/app/project/Exploratory%20Data%20Analysis.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:836\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=833'>834</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=834'>835</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=835'>836</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=836'>837</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=837'>838</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:819\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=815'>816</a>\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=816'>817</a>\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=817'>818</a>\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=818'>819</a>\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=819'>820</a>\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=820'>821</a>\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=821'>822</a>\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=823'>824</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=824'>825</a>\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py?line=825'>826</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:2923\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2920'>2921</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2921'>2922</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2922'>2923</a>\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2923'>2924</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2924'>2925</a>\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py?line=7183'>7184</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py?line=7184'>7185</a>\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py?line=7185'>7186</a>\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mUnknownError\u001b[0m: data/waymo/training_and_validation/segment-11967272535264406807_580_000_600_000_with_camera_labels.tfrecord; Input/output error [Op:IteratorGetNext]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "BATCH_SIZE_DISPLAY = 10\n",
    "batch = dataset.shuffle(BATCH_SIZE_DISPLAY, reshuffle_each_iteration=True)\n",
    "plt.figure()\n",
    "for idx in np.arange(BATCH_SIZE_DISPLAY):\n",
    "    for t in batch.take(1):\n",
    "        display_instances(t)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA\n",
    "\n",
    "In this last part, you are free to perform any additional analysis of the dataset. What else would like to know about the data?\n",
    "For example, think about data distribution. So far, you have only looked at a single file...\n",
    "\n",
    "Due to the size of the dataset, we will focus an a restricted batch of images(defined by the user) to perform data analysys such as:\n",
    "\n",
    ">- Class Distribution: used to identify if a class is more represented in the dataset. If yes, may or may not require a class compensation through Data Augmentation or Data Acquisition, depending of the use case.\n",
    ">- Number of instances/object per image: Allow to know if the dataset is based on scenes with high density objects and further to have better train/validation split, data augmentation, etc.\n",
    ">- Number of each class per image: can be usefull for data augmentation\n",
    ">- Image Brightness: can be usefull for data augmentation, weather robustness, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_ANALYSIS = 20000\n",
    "\n",
    "distri = {1: [], 2: [], 4: []}\n",
    "nb_obj_per_img = {\"max\" : 0, \"mean\" : 0, \"min\": 0}\n",
    "brightness = {\"max\" : [], \"mean\" : [], \"min\": []}\n",
    "from PIL import Image\n",
    "id = 0\n",
    "for idx,batch in enumerate(dataset.take(BATCH_SIZE_ANALYSIS)):\n",
    "    img = batch[\"image\"].numpy()\n",
    "    PIL_image = np.asarray(Image.fromarray(np.uint8(img)).convert('L'))\n",
    "    brightness[\"max\"].append(np.max(PIL_image))\n",
    "    brightness[\"min\"].append(np.min(PIL_image))\n",
    "    brightness[\"mean\"].append(np.mean(PIL_image))\n",
    "\n",
    "    classes = batch[\"groundtruth_classes\"].numpy().tolist()\n",
    "\n",
    "    for idx_distri in distri.keys():\n",
    "        distri[idx_distri].append(classes.count(idx_distri))\n",
    "\n",
    "    nb_obj_per_img[\"max\"] = max(nb_obj_per_img[\"max\"], len(classes))\n",
    "    nb_obj_per_img[\"min\"] = min(nb_obj_per_img[\"min\"], len(classes))\n",
    "    nb_obj_per_img[\"mean\"] = (((nb_obj_per_img[\"mean\"] + len(classes)) / 2) if idx!=0 else len(classes))\n",
    "    id += 1\n",
    "\n",
    "print(\"How many objects/instances per Image:\")\n",
    "print(f\"         Maximum Nb of Instances: {nb_obj_per_img['max']}\")\n",
    "print(f\"         Average Nb of Instances: {nb_obj_per_img['mean']}\")\n",
    "print(f\"         Minimum Nb of Instances: {nb_obj_per_img['min']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "print(f\"Mean Brightness : {np.mean(brightness['mean'])}\")\n",
    "fig.patch.set_facecolor('white')\n",
    "x = np.arange(len(brightness[\"mean\"]))\n",
    "plt.hist(brightness[\"mean\"],max(brightness[\"mean\"]).astype(np.int32))\n",
    "\n",
    "plt.ylabel('Nb images', fontsize=18)\n",
    "plt.title(f'Luminance Distribution over {BATCH_SIZE_ANALYSIS}', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.patch.set_facecolor('white')\n",
    "for idx, name in MAPPING_CLASS.items():\n",
    "    plt.bar(name, sum(distri[idx]), align='center', alpha=0.5, color=COLORMAP[idx])\n",
    "    print(f\"{name} represents {sum(distri[idx])}/{BATCH_SIZE_ANALYSIS} = {sum(distri[idx])*100/BATCH_SIZE_ANALYSIS}%\")\n",
    "\n",
    "plt.xticks(np.arange(len(MAPPING_CLASS.values())), MAPPING_CLASS.values(), fontsize=18)\n",
    "plt.ylabel('Instances', fontsize=18)\n",
    "plt.title(f'Classes Distribution over {BATCH_SIZE_ANALYSIS}', fontsize=18)\n",
    "tmp=[sum(distri[idx]) for idx in MAPPING_CLASS.keys()]\n",
    "for i, v in enumerate(tmp):\n",
    "    plt.text(i-.15, v/tmp[i]+100, f\"{tmp[i]} - {sum(distri[idx])*100/BATCH_SIZE_ANALYSIS}%\", fontsize=18, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,len(distri.keys()), figsize=(16,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "for idx in np.arange(len(distri.keys())):\n",
    "    key = list(distri.keys())[idx]\n",
    "    ax[idx].hist(distri[key], max(distri[key]))\n",
    "    ax[idx].set_title(MAPPING_CLASS[key])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
